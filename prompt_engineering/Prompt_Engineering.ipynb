{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58XL8olWy4Gt"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz_tLUjZzFm8",
        "outputId": "cf956051-2b80-490d-da1f-cc18d2182e6a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting setuptools>=42\n",
            "    Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "  Collecting scikit-build>=0.13\n",
            "    Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 1.7 MB/s eta 0:00:00\n",
            "  Collecting cmake>=3.18\n",
            "    Downloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 27.0 MB/s eta 0:00:00\n",
            "  Collecting ninja\n",
            "    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 22.6 MB/s eta 0:00:00\n",
            "  Collecting distro (from scikit-build>=0.13)\n",
            "    Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "  Collecting packaging (from scikit-build>=0.13)\n",
            "    Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 3.8 MB/s eta 0:00:00\n",
            "  Collecting tomli (from scikit-build>=0.13)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
            "    Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n",
            "    Creating /tmp/pip-build-env-2jxgzz6g/overlay/local/bin\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/wheel to 755\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/distro to 755\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-2jxgzz6g/overlay/local/bin/ctest to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "  Successfully installed cmake-3.29.2 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-69.5.1 tomli-2.0.1 wheel-0.43.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info\n",
            "  writing /tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-hyvbugi8/llama_cpp_python-0.1.78.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.23.4\n",
            "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m189.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Configuring done (0.8s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_cmake_test_compile/build\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja' generator - success\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  Configuring Project\n",
            "    Working directory:\n",
            "      /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "    Command:\n",
            "      /tmp/pip-build-env-2jxgzz6g/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-2jxgzz6g/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-2jxgzz6g/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-2jxgzz6g/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Configuring done (4.1s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
            "  [3/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  [4/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
            "  [7/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
            "  [8/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
            "  [8/9] Install the project...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
            "  -- Installing: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n",
            "\n",
            "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
            "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
            "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
            "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
            "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
            "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
            "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
            "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
            "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
            "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
            "  copying /tmp/pip-install-uspwizhh/llama-cpp-python_fa6fc0c9b9ce40359be98893f456d717/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copied 9 files\n",
            "  running build_ext\n",
            "  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copied 11 files\n",
            "  running install_data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  copied 0 files\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-yemqan43/.tmp-ecb_ha_k/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'llama_cpp/__init__.py'\n",
            "  adding 'llama_cpp/libllama.so'\n",
            "  adding 'llama_cpp/llama.py'\n",
            "  adding 'llama_cpp/llama_cpp.py'\n",
            "  adding 'llama_cpp/llama_grammar.py'\n",
            "  adding 'llama_cpp/llama_types.py'\n",
            "  adding 'llama_cpp/py.typed'\n",
            "  adding 'llama_cpp/utils.py'\n",
            "  adding 'llama_cpp/server/__init__.py'\n",
            "  adding 'llama_cpp/server/__main__.py'\n",
            "  adding 'llama_cpp/server/app.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
            "  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811184 sha256=07d75cf32e0dfb5c4d12a253e322cf6192c50602b34c55fa1098b5a2ac315376\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yn7w61lw/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.11.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  changing mode of /usr/local/bin/f2py3 to 755\n",
            "  changing mode of /usr/local/bin/f2py3.10 to 755\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.11.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Installazione delle librerie necessarie con specifiche per l'uso della GPU\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install openpyxl\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCrawMYnz2J0",
        "outputId": "65e60d4b-ef31-416a-b2f4-b96b0a7e56c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import spacy\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXLvFoRHz9wx"
      },
      "source": [
        "# Corpus Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVW0iox70HJv",
        "outputId": "8fc7284b-d744-4253-f50b-98f71f16608d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            CODE ID  Turn Participant  \\\n",
            "0  ctyHWUgv9B_kHA6DNcrgMxadmJZJ1sHa     1           U   \n",
            "1  ctyHWUgv9B_kHA6DNcrgMxadmJZJ1sHa     2           S   \n",
            "2  ctyHWUgv9B_kHA6DNcrgMxadmJZJ1sHa     3           U   \n",
            "3  ctyHWUgv9B_kHA6DNcrgMxadmJZJ1sHa     4           S   \n",
            "4  ctyHWUgv9B_kHA6DNcrgMxadmJZJ1sHa     5           U   \n",
            "\n",
            "                                                Text       Version Experiment  \\\n",
            "0                                                hi   [2023-05-28]       [Si]   \n",
            "1  I don't know, try asking something like, \"What...  [2023-05-28]       [Si]   \n",
            "2                            describe the automaton   [2023-05-28]       [Si]   \n",
            "3  It is an automaton which accepts zero or more ...  [2023-05-28]       [Si]   \n",
            "4           There is a transition between q2 and q0   [2023-05-28]       [Si]   \n",
            "\n",
            "  VIP  Token count                 Errors_GOLD  \n",
            "0  no            1                         NaN  \n",
            "1  no           13                Topic change  \n",
            "2  no            3  Ignoring question/feedback  \n",
            "3  no           86                         NaN  \n",
            "4  no            8                         NaN  \n"
          ]
        }
      ],
      "source": [
        "# Load the Excel file\n",
        "data_path = '/content/drive/My Drive/Llama2_Translation_LNC/interaction-corpus.xlsx'\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otbgFgP51CLw",
        "outputId": "d3cbf899-2b80-484b-8e7a-1682b78c6d01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Errors_GOLD\n",
              "Topic change                                116\n",
              "Straight wrong response                      28\n",
              "Indirect response                            16\n",
              "Excess of information                        13\n",
              "Lack of information                           9\n",
              "Indirect response; Topic change               3\n",
              "Excess of information; Indirect response      1\n",
              "Ignoring question/feedback                    1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Filter the data for system responses only\n",
        "system_responses = data[data['Participant'] == 'S']\n",
        "\n",
        "# Count the types of errors for system responses\n",
        "system_error_counts = system_responses['Errors_GOLD'].value_counts()\n",
        "system_error_counts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data for system responses only\n",
        "user_responses = data[data['Participant'] == 'U']\n",
        "\n",
        "# Count the types of errors for system responses\n",
        "system_error_counts = user_responses['Errors_GOLD'].value_counts()\n",
        "system_error_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gRFI14JqEbR",
        "outputId": "54260974-aaa4-4caf-d161-57b2f1e52283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Errors_GOLD\n",
              "Repetition                                        43\n",
              "Ignoring question/feedback                        21\n",
              "Grammatical error                                 18\n",
              "Ignoring question/feedback; Repetition            13\n",
              "Off-topic                                          4\n",
              "Non-understandable                                 4\n",
              "Lack of information                                4\n",
              "Non-cooperativity                                  3\n",
              "Ignoring question/feedback; Non-understandable     2\n",
              "Ill-formed                                         2\n",
              "Ignoring question/feedback; Off-topic              1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escludiamo:\n",
        "1. Off-topic\n",
        "3. Non-understandable\n",
        "4. Non-cooperativity\n",
        "5. Repetition"
      ],
      "metadata": {
        "id": "GOTcPQR3s1ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Change"
      ],
      "metadata": {
        "id": "MfiKSzSKp1AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Topic change\" error\n",
        "topic_change_indices = system_responses[system_responses['Errors_GOLD'] == 'Topic change'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Topic Change\"\n",
        "user_inputs_before_topic_change = data.loc[topic_change_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_tc = user_inputs_before_topic_change[\n",
        "    user_inputs_before_topic_change['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_topic_change['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_tc = filtered_user_inputs_tc['Text'].sample(15).tolist()\n",
        "final_user_queries_tc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJEMw1EFv31M",
        "outputId": "6d2b2922-6c10-45ba-d173-2c9e2a059bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Talk me about transitions\\xa0',\n",
              " 'is there any self loop',\n",
              " 'What transitions does the automaton have\\xa0',\n",
              " 'how many 0 art here\\xa0',\n",
              " 'What is an example of accepted string\\xa0',\n",
              " 'What could be a minimal spatial representation for this automaton\\xa0',\n",
              " 'Summarise the automaton\\xa0',\n",
              " 'What are the accepted inputs\\xa0',\n",
              " 'what is the regular expression describing the automata\\xa0',\n",
              " 'How can I define an automaton\\xa0',\n",
              " 'hi\\xa0',\n",
              " 'what is the input',\n",
              " 'There is a transition between q5 and q7\\xa0',\n",
              " 'Does it only accept 1s and 0s\\xa0',\n",
              " 'what is the initial state of the automaton\\xa0']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_tc)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3O4cG0zw2Q-",
        "outputId": "bc7e27cf-e17a-4e40-e78b-66020bf11954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La lunghezza dell'array è: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Straight Wrong Response"
      ],
      "metadata": {
        "id": "CtC0h53Kp7cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Straight wrong response\" error\n",
        "straight_wrong_response_indices = system_responses[system_responses['Errors_GOLD'] == 'Straight wrong response'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Straight wrong response\"\n",
        "user_inputs_before_straight_wrong_response = data.loc[straight_wrong_response_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_swr = user_inputs_before_straight_wrong_response[\n",
        "    user_inputs_before_straight_wrong_response['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_straight_wrong_response['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_swr = filtered_user_inputs_swr['Text'].sample(10).tolist()\n",
        "final_user_queries_swr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQju8EP7govb",
        "outputId": "f3fab89b-5e35-4eb1-cd16-7f847bc2e20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How many states does a pentagon shaped automaton have\\xa0',\n",
              " 'arc from q2 to q1',\n",
              " 'how many arcs marked by 1 are there\\xa0',\n",
              " 'q2',\n",
              " 'Which transitions end in q0\\xa0',\n",
              " 'Is there an arc between q0 and q1\\xa0',\n",
              " 'If q2 is the final state the language accepted by the automaton is the same\\xa0',\n",
              " 'transitions',\n",
              " 'Is there an arc between q1 and q2\\xa0',\n",
              " 'q0']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_swr)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJqTmkb_pbdr",
        "outputId": "694b2bd4-38e7-4670-c373-f3ffe09e39b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La lunghezza dell'array è: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_swr['Text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf_JViCzvpdy",
        "outputId": "e6d6e8c0-3122-4454-c165-566df2f0bd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the states\\xa0',\n",
              " 'If q2 is the final state the language accepted by the automaton is the same\\xa0',\n",
              " 'is q4 linked to q0\\xa0',\n",
              " 'Is there an arc from q4 to q0\\xa0',\n",
              " 'If q2 were the final state the language accepted by the automaton is the same\\xa0',\n",
              " 'How many states can I use in an automaton\\xa0',\n",
              " 'Which transitions end in q0\\xa0',\n",
              " 'is there an arc between q0 and q1\\xa0',\n",
              " 'How many states does a pentagon shaped automaton have\\xa0',\n",
              " 'Is there an arc between q0 and q1\\xa0',\n",
              " 'Is there an arc between q1 and q2\\xa0',\n",
              " 'is there an arc from q2 to q1\\xa0',\n",
              " 'If q2 were the final state the language accepted by the automaton is the same\\xa0',\n",
              " 'If q0 were the final state the language accepted by the automaton is the same\\xa0',\n",
              " 'transitions',\n",
              " 'arcs between q4 and q0',\n",
              " 'arc from q2 to q1',\n",
              " 'how many arcs marked by 1 are there\\xa0',\n",
              " 'describe the shape of the automaton',\n",
              " 'does the automaton accept other languages',\n",
              " 'q0',\n",
              " 'q2',\n",
              " 'q3 to q2',\n",
              " 'q4 to q0',\n",
              " 'describe the position of nodes']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indirect Response"
      ],
      "metadata": {
        "id": "yC7agCVwqDEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Indirect response\" error\n",
        "indirect_response_indices = system_responses[system_responses['Errors_GOLD'] == 'Indirect response'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Indirect response\"\n",
        "user_inputs_before_indirect_response = data.loc[indirect_response_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_ir = user_inputs_before_indirect_response[\n",
        "    user_inputs_before_indirect_response['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_indirect_response['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_ir = filtered_user_inputs_ir['Text'].sample(8).tolist()\n",
        "final_user_queries_ir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUSmiG0Zip1C",
        "outputId": "13feeae9-c675-48e9-80dd-e9cd387fd8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What happens when a 0 comes to state q1\\xa0',\n",
              " 'The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'is there an arc from q0 to q1\\xa0',\n",
              " 'q0 to q4',\n",
              " 'Is 11100 part of the accepted language\\xa0',\n",
              " 'The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'is q0 linked to q0\\xa0',\n",
              " 'Is there a transition from q0 to q5\\xa0']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_ir)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzrkQ4sIpe8X",
        "outputId": "afbe85d7-7c27-4a1c-8419-aee091797e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La lunghezza dell'array è: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_ir['Text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x9BYaZb4PHj",
        "outputId": "b4b54de0-d045-440f-a7b1-10e7dfc50b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'What happens when a 0 comes to state q1\\xa0',\n",
              " 'is q0 linked to q0\\xa0',\n",
              " 'is q1 linked to q4\\xa0',\n",
              " 'Is 11100 part of the accepted language\\xa0',\n",
              " 'Is there a transition from q0 to q5\\xa0',\n",
              " 'is q4 a final state\\xa0',\n",
              " 'Is there a transition from q0 to q5\\xa0',\n",
              " 'The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'Is there a transition from q0 to q5\\xa0',\n",
              " 'The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'is there an arc from q0 to q1\\xa0',\n",
              " 'The automaton accepts a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'Does the automaton accept a language allowing words made of an odd number of 0s and 1s\\xa0',\n",
              " 'how many transitions',\n",
              " 'q0 to q4']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excess of Information"
      ],
      "metadata": {
        "id": "bN9ns0frqNwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Excess of information\" error\n",
        "excess_of_information_indices = system_responses[system_responses['Errors_GOLD'] == 'Excess of information'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Excess of information\"\n",
        "user_inputs_before_excess_of_information = data.loc[excess_of_information_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_eoi = user_inputs_before_excess_of_information[\n",
        "    user_inputs_before_excess_of_information['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_excess_of_information['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_eoi = filtered_user_inputs_eoi['Text'].sample(4).tolist()\n",
        "final_user_queries_eoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH45Lb57cFF3",
        "outputId": "9d33f801-82fa-4181-9a66-ee6c76d7791c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How many transitions start from q0\\xa0',\n",
              " 'Please describe the transitions\\xa0',\n",
              " 'describe the arcs',\n",
              " 'can you describe the transitions']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_eoi)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj25QnJbpiSj",
        "outputId": "c74f0c06-9579-41b1-ec41-3cd3a5210e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La lunghezza dell'array è: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_eoi['Text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3t-1EWN52dK",
        "outputId": "bd08b5e4-231c-4df6-ba4e-caa24cfada23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can you describe the transitions\\xa0',\n",
              " 'Please describe the transitions\\xa0',\n",
              " 'How many transitions start from q0\\xa0',\n",
              " 'how many transitions\\xa0',\n",
              " 'how is marked the arc between q0 and q2\\xa0',\n",
              " 'which is final state',\n",
              " 'label from q3 to q4',\n",
              " 'describe the arcs',\n",
              " 'can you describe the transitions']"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lack of Information"
      ],
      "metadata": {
        "id": "kLQs11WDqUhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Lack of information\" error\n",
        "lack_of_information_indices = system_responses[system_responses['Errors_GOLD'] == 'Lack of information'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Lack of information\"\n",
        "user_inputs_before_lack_of_information = data.loc[lack_of_information_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_loi = user_inputs_before_lack_of_information[\n",
        "    user_inputs_before_lack_of_information['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_lack_of_information['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_loi = filtered_user_inputs_loi['Text'].sample(4).tolist()\n",
        "final_user_queries_loi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWBAxP_ojJCC",
        "outputId": "4481d3bd-3076-4067-c27a-f008fe4dc68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What transitions enter and exit q4\\xa0',\n",
              " 'are there arcs between q2 and q0\\xa0',\n",
              " 'What transitions enter and exit q2\\xa0',\n",
              " 'Briefly describe the automaton How many states are there\\xa0']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_loi)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9eFISkEpmwA",
        "outputId": "60d78266-fe11-44af-9228-090f36341bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La lunghezza dell'array è: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_loi['Text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEZVIIqR552k",
        "outputId": "bacbd6ed-602d-48ed-c738-b1a93561247e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There is a transition between q2 and q0\\xa0',\n",
              " 'What transitions enter and exit q0\\xa0',\n",
              " 'What transitions enter and exit q1\\xa0',\n",
              " 'What transitions enter and exit q2\\xa0',\n",
              " 'What transitions enter and exit q4\\xa0',\n",
              " 'What transitions enter and exit q3\\xa0',\n",
              " 'are there arcs between q2 and q0\\xa0',\n",
              " 'Briefly describe the automaton How many states are there\\xa0',\n",
              " 'Is there a transition from q0 to q5\\xa0']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdrW2k_4wNRf"
      },
      "source": [
        "# Chiamata al modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "7554cfc8ce65465b98da9c69c126413e",
            "7211f954ae4c4cbab701a26718c49bbc",
            "a77067eaecd545c689023726d4f317e5",
            "ffcca6087acb4601a4f22378281b68cd",
            "e331a08a1174423291a16f0211d23282",
            "22887fb8a161426f82d774aed4631401",
            "f6de9c5b2e684ae99f2ea1270a850321",
            "a77f38feb0bb4653857e8fa0db83c0c1",
            "f83e568dcf824fc8adeaac9a5617b397",
            "63601f51389b417e90005a2e6838631d",
            "1d8df51bb1534b599c40b4c45a546fab"
          ]
        },
        "id": "0P5evo50wZLk",
        "outputId": "444f67f2-b889-4046-f28e-36b3d5a75394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7554cfc8ce65465b98da9c69c126413e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Seleziono LLama2 13B come modello\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "# Download del modello da Hugging Face Hub\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# Inizializzazione del modello con configurazione per GPU\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # Numero di core della CPU\n",
        "    n_batch=512,  # Numero di batch, dipende dalla VRAM della GPU\n",
        "    n_gpu_layers=32  # Numero di layer gestiti dalla GPU\n",
        ")\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-SlHHjXwdaC"
      },
      "outputs": [],
      "source": [
        "# Generazione della risposta\n",
        "def call_model(prompt):\n",
        "  response = lcpp_llm(prompt=prompt, max_tokens=4096)\n",
        "  #print(response[\"choices\"][0][\"text\"])\n",
        "  return response[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwOMqxWiTZng"
      },
      "source": [
        "# Salvataggio Dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBBHDd4npeD"
      },
      "outputs": [],
      "source": [
        "def setup_xlsx(file_path):\n",
        "    # Controlla se il file esiste\n",
        "    if not os.path.exists(file_path):\n",
        "        # Crea un nuovo workbook e aggiunge un foglio\n",
        "        wb = openpyxl.Workbook()\n",
        "        ws = wb.active\n",
        "        ws.title = \"Data\"\n",
        "        ws.append(['Input', 'Translation', 'Output', 'Type', 'Error'])  # Aggiungi intestazioni\n",
        "        wb.save(file_path)\n",
        "    else:\n",
        "        # Apri il workbook esistente\n",
        "        wb = openpyxl.load_workbook(file_path)\n",
        "    return wb\n",
        "\n",
        "def append_to_xlsx(wb, file_path, data):\n",
        "    # Carica il foglio di lavoro\n",
        "    ws = wb.active\n",
        "    # Aggiungi i nuovi dati\n",
        "    ws.append(data)\n",
        "    # Salva il workbook\n",
        "    wb.save(file_path)\n",
        "\n",
        "# Percorso del file Excel\n",
        "filename = '/content/drive/My Drive/Llama2_Translation_LNC/Llama2InteractionCorpus_01_05_IR_EOI_LOI.xlsx'\n",
        "\n",
        "# Setup iniziale del file Excel\n",
        "wb = setup_xlsx(filename)\n",
        "\n",
        "def update_excel(input_text, translation, output, prompt_type, error_type):\n",
        "    # Dati da aggiungere\n",
        "    data = [input_text, translation, output, prompt_type, error_type]\n",
        "    # Aggiorna il file Excel\n",
        "    append_to_xlsx(wb, filename, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNBshkVc50JZ"
      },
      "source": [
        "# Prompt Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_between_brackets(input_string):\n",
        "    # Usa un'espressione regolare per trovare il testo tra < e >\n",
        "    match = re.search(r'<([^>]*)>', input_string)\n",
        "    # Se c'è una corrispondenza, restituisci il gruppo catturato, altrimenti restituisci None o una stringa vuota\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return \"Output not found!\""
      ],
      "metadata": {
        "id": "4HSWwSm8sUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNUyD0Rk8iPd"
      },
      "outputs": [],
      "source": [
        "llama2PromptTemplate = lambda userPrompt,systemPrompt: f\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "{systemPrompt}\n",
        "<</SYS>>\n",
        "\n",
        "{userPrompt} [/INST]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ794IHY7AN3"
      },
      "outputs": [],
      "source": [
        "systemPrompt=\"\"\"\n",
        "You are an assistant expert on finite state automata. Your task is to translate \\\n",
        "user inputs from natural language to a controlled natural language format. \\\n",
        "This translation should adhere to specific vocabulary and syntax guidelines \\\n",
        "suitable for an AIML chatbot. Your response must be precise, using technical \\\n",
        "terminology related to finite state automata where appropriate. Ensure the \\\n",
        "translated text contains clear, unambiguous language that aligns with the \\\n",
        "rules governing the AIML chatbot's response system. Focus on maintaining the \\\n",
        "integrity of the technical content while simplifying the explanation for \\\n",
        "accurate and effective chatbot communication.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYAM_B577Gc4"
      },
      "outputs": [],
      "source": [
        "userPromptZeroShot = lambda text: f\"\"\"\n",
        "  Translate the text delimited by triple backticks, without any preambles or \\\n",
        "  additional explanation, into controlled natural language sentence suitable \\\n",
        "  for an AIML system.\n",
        "  Format the response by directly placing the translation within angle \\\n",
        "  brackets < >.\n",
        "  For example:\n",
        "  Output: <Your text here>.\n",
        "  Now, use the same format for the translation:\n",
        "  ``` {text} ```\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptChainOfThoughts = lambda text: f\"\"\"\n",
        "Translate '{text}' into controlled natural language for an AIML chatbot, focusing on:\n",
        "1. Identifying key components related to finite state automata.\n",
        "2. Using precise, technical terminology where appropriate.\n",
        "3. Ensuring clarity and adherence to AIML chatbot rules.\n",
        "Ensure that only the response itself is literally placed between '<' and '>'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y6XRaPOvtv3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptChainOfThoughts = lambda text: f\"\"\"\n",
        "Translate the user's input considering the following steps:\n",
        "1. Analyze the user input for key components:\n",
        "   Input: '{text}'\n",
        "2. Identify the critical elements and any technical terminology related to finite state automata:\n",
        "   - Examine keywords and phrases that signify specific automata concepts.\n",
        "3. Construct a response in controlled natural language:\n",
        "   - Determine how to express these components clearly and unambiguously.\n",
        "   - Use controlled language structures that fit the AIML chatbot's rules.\n",
        "4. Synthesize the translation to ensure clarity, precision, and adherence to technical terms.\n",
        "5. Finalize the translation ensuring technical accuracy and alignment with AIML standards:\n",
        "   - Format the response to be enclosed within angular brackets.\n",
        "6. Explicit instruction for output:\n",
        "   - Example format for final output: 'Please format the final response like this: <RESPONSE>'\n",
        "   - Ensure that only the response itself is literally placed between '<' and '>'.\n",
        "Example:\n",
        "Input: \"Tell me a little bit about the automaton \"\n",
        "Output: \"<Describe me briefly the automaton>\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DuEx1bRpRqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptZeroChainOfThoughts = lambda text: f\"\"\"\n",
        "  Translate the text delimited by triple backticks, without any preambles or \\\n",
        "  additional explanation, into controlled natural language sentence suitable \\\n",
        "  for an AIML system.\n",
        "  Format the response by directly placing the translation within angle \\\n",
        "  brackets < >.\n",
        "  For example:\n",
        "  Output: <Your text here>.\n",
        "  Now the same format for the translation:\n",
        "  ``` {text} ```\n",
        "  Take a deep breath and work on this problem step-by-step.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "atVI6bXsvptw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Change"
      ],
      "metadata": {
        "id": "sDJwiSnWvaSs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viec3G8N6AKu"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPJ02sJ__UCW",
        "outputId": "190e6038-1314-42f5-86a8-21944aaf1700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Tell me a little bit about the automaton\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Tell me about the finite state automaton. Please provide information on its syntax, vocabulary, and any relevant technical details.\n"
          ]
        }
      ],
      "source": [
        "text = \"Tell me a little bit about the automaton\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK7L-lIN-J0n",
        "outputId": "6b4d730e-e65b-41fc-ff71-2c2d26870350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Talk me about transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Talk me about transitions\n",
            "\n",
            "Input: is there any self loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  Does the finite state automaton contain a self-loop? \n",
            "\n",
            "Input: What transitions does the automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions does the automaton have?\n",
            "\n",
            "Input: how many 0 art here \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many zeroes are present in the given string?\n",
            "\n",
            "Input: What is an example of accepted string \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is an example of a finite state automaton that accepts the string \"01101010\"?\n",
            "\n",
            "Input: What could be a minimal spatial representation for this automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What could be a minimal spatial representation for this finite state automaton?\n",
            "\n",
            "Input: Summarise the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Summarize the finite state automaton\n",
            "\n",
            "Input: What are the accepted inputs \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the accepted inputs?\n",
            "\n",
            "Input: what is the regular expression describing the automata \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the regular expression describing the finite state automaton?\n",
            "\n",
            "Input: How can I define an automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How can I define an automaton to recognize a specific language?\n",
            "\n",
            "Input: hi \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: hi\n",
            "\n",
            "Input: what is the input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: input\n",
            "\n",
            "Input: There is a transition between q5 and q7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: There exists a transition between state q5 and state q7\n",
            "\n",
            "Input: Does it only accept 1s and 0s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does it only accept 1s and 0s?\n",
            "\n",
            "Input: what is the initial state of the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the initial state of the finite state automaton?\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "HD85Wby4DELy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotTC = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Tell me a little bit about the automaton \"\n",
        "Output: \"<Describe me briefly the automaton>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "96tx-ydwV2if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851afb2e-7e4c-4ce5-c2d6-f70c99fcc01a",
        "id": "u1WN_t48y6t5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what are the states and the connections between states\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the states and the transitions between them\n"
          ]
        }
      ],
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotTC(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bb8d83-7a44-48d4-a125-d2d2d4c25ea6",
        "id": "jw4Uefxny6uD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Talk me about transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the transitions of the automaton\n",
            "\n",
            "Input: is there any self loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does this automaton have a self-loop?\n",
            "\n",
            "Input: What transitions does the automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the transitions present in the automaton?\n",
            "\n",
            "Input: how many 0 art here \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Quantify the number of zero-states present in the given automaton?\n",
            "\n",
            "Input: What is an example of accepted string \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Provide an example of a string that is accepted by a finite state automaton?\n",
            "\n",
            "Input: What could be a minimal spatial representation for this automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What would be the most minimal spatial representation for this finite state machine?\n",
            "\n",
            "Input: Summarise the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Provide a concise overview of the automaton, highlighting its key components and behavior.\n",
            "\n",
            "Input: What are the accepted inputs \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the set of inputs that the automaton accepts\n",
            "\n",
            "Input: what is the regular expression describing the automata \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the regular expression used to define the automata\n",
            "\n",
            "Input: How can I define an automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the process for defining an automaton\n",
            "\n",
            "Input: hi \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Greet me briefly?\n",
            "\n",
            "Input: what is the input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe me the input\n",
            "\n",
            "Input: There is a transition between q5 and q7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the transition between state q5 and state q7\n",
            "\n",
            "Input: Does it only accept 1s and 0s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is the automaton limited to accepting only 1s and 0s?\n",
            "\n",
            "Input: what is the initial state of the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the initial state of the automaton\n"
          ]
        }
      ],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotTC(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "kWgIqGK0f8bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:\n",
        "Input: \"Tell me a little bit about the automaton\"\n",
        "Output: \"Describe me briefly the automaton>\"\n",
        "\n",
        "Example 2:\n",
        "Input: \"According to your understanding of Automaton 2...\"\n",
        "Output: \"Tell me more about the automaton 2>\"\n",
        "\n",
        "Example 3:\n",
        "Input: \"What is the final state of an automaton\"\n",
        "Output: \"Identify the final state of the automaton>\"\n",
        "\n",
        "Example 4:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"What are the transitions>\"\n",
        "\n",
        "Example 5:\n",
        "Input: \"How is an automaton defined\"\n",
        "Output: \"What is an automaton>\"\n",
        "\n",
        "Example 6:\n",
        "Input: \"what are the symbols accepted by the automaton\"\n",
        "Output: \"Specify the alphabet used by the automaton.>\"\n",
        "\n",
        "Example 7:\n",
        "Input: \"nodes\"\n",
        "Output: \"Tell me more about states>\"\n",
        "\n",
        "Example 8:\n",
        "Input: \"What are the transitions\"\n",
        "Output: \"What are transitions>\"\n",
        "\n",
        "Example 9:\n",
        "Input: \"Give me a list of the arcs\"\n",
        "Output: \"What are arcs>\"\n",
        "\n",
        "Example 10:\n",
        "Input: \"what is the initial stage of the automaton\"\n",
        "Output: \"What is the initial state>\""
      ],
      "metadata": {
        "id": "KASw9-nH_eI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsTC = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"According to your understanding of Automaton 2...\"\n",
        "Output: \"<Tell me more about the automaton 2>\"\n",
        "Example 2:\n",
        "Input: \"What is the final state of an automaton\"\n",
        "Output: \"<Identify the final state of the automaton>\"\n",
        "Example 3:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"<What are the transitions>\"\n",
        "Example 4:\n",
        "Input: \"How is an automaton defined\"\n",
        "Output: \"<What is an automaton>\"\n",
        "Example 5:\n",
        "Input: \"what are the symbols accepted by the automaton\"\n",
        "Output: \"<Specify the alphabet used by the automaton.>\"\n",
        "Example 6:\n",
        "Input: \"nodes\"\n",
        "Output: \"<Tell me more about states>\"\n",
        "Example 7:\n",
        "Input: \"Give me a list of the arcs\"\n",
        "Output: \"<What are arcs>\"\n",
        "Example 8:\n",
        "Input: \"what is the initial stage of the automaton\"\n",
        "Output: \"<What is the initial state>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bfi8urM5W-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"According to your understanding of Automaton 2 what is its optimal spatial representation\"\n",
        "print(\"\\nInput: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsTC(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "__KPGGfSoVMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46177e85-d932-4834-f656-21aa21d2551d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: According to your understanding of Automaton 2 what is its optimal spatial representation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Tell me more about the optimum spatial representation of Automaton 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsTC(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "xx6CV0dcAgsF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3aaeac1-940e-44cd-8d87-152a1355d105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Talk me about transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the transitions\n",
            "\n",
            "Input: is there any self loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Tell me if there are any self-loops in the automaton\n",
            "\n",
            "Input: What transitions does the automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the transitions of the automaton?\n",
            "\n",
            "Input: how many 0 art here \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Tell me more about the number of zeroes you are referring to.\n",
            "\n",
            "Input: What is an example of accepted string \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Specify an example of a string that the automaton accepts.\n",
            "\n",
            "Input: What could be a minimal spatial representation for this automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Specify the minimal spatial representation of the automaton\n",
            "\n",
            "Input: Summarise the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Tell me the summary of the automaton\n",
            "\n",
            "Input: What are the accepted inputs \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Specify the alphabet used by the automaton.\n",
            "\n",
            "Input: what is the regular expression describing the automata \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the regular expression representing the automaton\n",
            "\n",
            "Input: How can I define an automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Specify the definition of the automaton\n",
            "\n",
            "Input: hi \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Hi there!\n",
            "\n",
            "Input: what is the input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Specify the input\n",
            "\n",
            "Input: There is a transition between q5 and q7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the transitions between states q5 and q7?\n",
            "\n",
            "Input: Does it only accept 1s and 0s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: what is the initial state of the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Identify the initial state of the automaton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "xPmkMLVcwyRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "print(\"Completed Output: \" + model_output)\n",
        "#update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWfdFzZQygQ",
        "outputId": "6d73f778-eea1-4b6b-8315-104e4c5d61d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what are the states and the connections between states\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: state\n",
            "Completed Output: <state>  A finite state automaton (FSA) consists of a set of states and transitions between them.</state>\n",
            "\n",
            "These states can be thought of as distinct conditions or situations in which the FSA may reside, and the connections or transitions represent the possible ways for the FSA to move from one state to another based on input received. For example, consider an FSA that recognizes the language {a, b}. The states in this case could be Q0 (the starting state), Q1, and Q2 (the final state). The transitions between them would look like this:\n",
            "\n",
            "Q0 -> a Q1\n",
            "Q0 -> b Q2\n",
            "Q1 -> a Q1\n",
            "Q1 -> ε Q0 (empty transition)\n",
            "Q2 -> ε Q0</transition>\n",
            "\n",
            "This FSA can recognize the language {a, b} by starting in Q0 and shifting to Q1 on input 'a', to Q2 on input 'b', and back to Q0 on input ''.</transition>\n",
            "\n",
            "By precisely identifying key components of finite state automata and utilizing technical terminology where appropriate, the chatbot can effectively communicate complex ideas with clarity and accuracy. The controlled natural language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "_6Zy-ZkrBh8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ebe0e9-d602-44dd-8969-ebdea3ba8689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Talk me about transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: is there any self loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: What transitions does the automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: how many 0 art here \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: What is an example of accepted string \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: What could be a minimal spatial representation for this automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Summarise the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: What are the accepted inputs \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: what is the regular expression describing the automata \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: How can I define an automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: hi \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: what is the input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: There is a transition between q5 and q7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Does it only accept 1s and 0s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: what is the initial state of the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "sNpQguQPw8zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h8mNO-fYPvv",
        "outputId": "9e93ba7f-3df6-4512-b3e7-d835a816d73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what are the states and the connections between states\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output What are the current states and their transitions?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "jdTP_25rCJ-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4e30fa-c08b-4e3c-bd81-a278b1b18f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Talk me about transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Talk me about transitions\n",
            "\n",
            "Input: is there any self loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there any self-loop in the finite state automaton?\n",
            "\n",
            "Input: What transitions does the automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions does the automaton have?\n",
            "\n",
            "Input: how many 0 art here \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many 0s are there in \"here\"?\n",
            "\n",
            "Input: What is an example of accepted string \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are some examples of accepted strings in finite state automata?\n",
            "\n",
            "Input: What could be a minimal spatial representation for this automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What could be a minimal spatial representation for this automaton?\n",
            "\n",
            "Input: Summarise the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the finite state automaton\n",
            "\n",
            "Input: What are the accepted inputs \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the accepted inputs?\n",
            "\n",
            "Input: what is the regular expression describing the automata \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the regular expression describing the finite state automaton?\n",
            "\n",
            "Input: How can I define an automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How can I define an automaton?\n",
            "\n",
            "Input: hi \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: hi\n",
            "\n",
            "Input: what is the input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: input\n",
            "\n",
            "Input: There is a transition between q5 and q7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: There is a transition between state q5 and state q7\n",
            "\n",
            "Input: Does it only accept 1s and 0s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does it only accept 1s and 0s?\n",
            "\n",
            "Input: what is the initial state of the automaton \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the initial state of the finite automaton?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Straight Wrong Response"
      ],
      "metadata": {
        "id": "docJqTtpCncC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuUmnbmbCncD"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e865a02b-58c2-477a-ace9-dec904d6a86d",
        "id": "lx3zPmmQCncD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Is there an arc between q0 and q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition from state q0 to state q1?\n",
            "Update done!\n"
          ]
        }
      ],
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eaa37e-4bf2-43c6-a0ef-7f57af10df61",
        "id": "IvHMe5kXCncE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many states does a pentagon shaped automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many states does a pentagon-shaped automaton have?\n",
            "\n",
            "Input: arc from q2 to q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: arc-transition from-state=\"q2\" to-state=\"q1\"\n",
            "\n",
            "Input: how many arcs marked by 1 are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: There are no arcs marked by 1.\n",
            "\n",
            "Input: q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: I have a finite state machine with a starting state S0 and two final states S1 and S2. I wish to identify the shortest route from S0 to either S1 or S2.\n",
            "\n",
            "Input: Which transitions end in q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Which transitions end in state q0?\n",
            "\n",
            "Input: Is there an arc between q0 and q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition from state q0 to state q1?\n",
            "\n",
            "Input: If q2 is the final state the language accepted by the automaton is the same \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: If q2 is the final state, the language accepted by the automaton is the same\n",
            "\n",
            "Input: transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: transitions\n",
            "\n",
            "Input: Is there an arc between q1 and q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there an edge/arc connecting state q1 to state q2?\n",
            "\n",
            "Input: q0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: q0\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "sXKvC5CfCncF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotSWR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"is q4 linked to q0\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zcW104EeCncF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74086f86-34e0-46bb-d89a-49f49c5eee30",
        "id": "f_bmHm3PCncF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Is there an arc between q0 and q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there any transitions from q0 to q1?\n"
          ]
        }
      ],
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotSWR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcaa0d6-10d5-41b8-ed75-62025b9c5bb6",
        "id": "dGAI08GfCncG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many states does a pentagon shaped automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the number of states in a pentagon-shaped finite state machine?\n",
            "\n",
            "Input: arc from q2 to q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does a directed arc exist from state q2 to state q1?\n",
            "\n",
            "Input: how many arcs marked by 1 are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the number of outgoing transitions with label '1'\n",
            "\n",
            "Input: q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a state with the ID \"q2\"\n",
            "\n",
            "Input: Which transitions end in q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the final states reached through transitions starting from q0\n",
            "\n",
            "Input: Is there an arc between q0 and q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does a transition exist between state q0 and state q1?\n",
            "\n",
            "Input: If q2 is the final state the language accepted by the automaton is the same \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a final state where the language accepted by the automaton remains unchanged?\n",
            "\n",
            "Input: transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there any state transitions available in the current FSA?\n",
            "\n",
            "Input: Is there an arc between q1 and q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does a transition exist between state q1 and state q2?\n",
            "\n",
            "Input: q0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is the initial state\n"
          ]
        }
      ],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotSWR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "CH5YWYmUCncG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsSWR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"is q4 linked to q0\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "Example 2:\n",
        "Input: \"transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "Example 3:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"<What are the transitions>\"\n",
        "Example 4:\n",
        "Input: \"arc from q2 to q1\"\n",
        "Output: \"<Is there a transition between q2 and q1>\"\n",
        "Example 5:\n",
        "Input: \"q0\"\n",
        "Output: \"<What is q0>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rMQKEyE1CncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsSWR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f0188f-02d5-40cc-9bef-d7f252a42838",
        "id": "IKsW9IcOCncH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you with that. Here's the translation of your input into controlled natural language format:\n",
            "\n",
            "Input: what are the states and the connections between states\n",
            "\n",
            "Output: <Specify the states and their transitions>\n",
            "\n",
            "In\n",
            "According to your understanding of Automaton 2 what is its optimal spatial representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsSWR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "9Xgl_IqsCncH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345c1238-4f8a-445b-c675-0f95ee3a5a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many states does a pentagon shaped automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the number of states in a pentagonal finite state automaton?\n",
            "\n",
            "Input: arc from q2 to q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition between q2 and q1\n",
            "\n",
            "Input: how many arcs marked by 1 are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the number of arcs labeled with 1?\n",
            "\n",
            "Input: q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a state with the ID q2?\n",
            "\n",
            "Input: Which transitions end in q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the transitions leading to q0?\n",
            "\n",
            "Input: Is there an arc between q0 and q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there any transitions between q0 and q1\n",
            "\n",
            "Input: If q2 is the final state the language accepted by the automaton is the same \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does the final state q2 accept the same language?\n",
            "\n",
            "Input: transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are transitions\n",
            "\n",
            "Input: Is there an arc between q1 and q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does there exist a transition between q1 and q2\n",
            "\n",
            "Input: q0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is q0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "S9fvnnBlCncH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ad6e3-4596-46c5-fb55-f55fc9d5c9b1",
        "id": "KF1X8qpPCncH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you translate the user's input into a controlled natural language format suitable for an AIML chatbot. Here's my analysis and translation of the input:\n",
            "\n",
            "1. Analyze the user input for key components:\n",
            "Input: \"According to your understanding of Automaton 2 what is its optimal spatial representation?\"\n",
            "\n",
            "Keywords and phrases that signify specific automata concepts:\n",
            "\n",
            "* Automaton 2\n",
            "* Optimal spatial representation\n",
            "\n",
            "2. Identify critical elements and technical terminology\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "f_sdogSLCncI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4033514-39d4-4a50-fc18-2851948a5087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many states does a pentagon shaped automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: arc from q2 to q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: how many arcs marked by 1 are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Which transitions end in q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Is there an arc between q0 and q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: If q2 is the final state the language accepted by the automaton is the same \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Is there an arc between q1 and q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: q0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "pGkwf5upCncI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fef8cb-9f6c-4d88-cfaa-7e38c3525a49",
        "id": "Ns6gQ70HCncI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your input into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<What are the states and the connections between states?>\n",
            "\n",
            "In the context of finite state automata, a \"state\" refers to a specific condition or status that the system can be in. These states are typically represented by nodes or circles in a diagram, and they are connected by arrows or transitions that represent the possible movements or changes between those states. The connections between states are called \"edges\" or \"transitions.\"\n",
            "\n",
            "For example, a finite state automaton might have the following states:\n",
            "\n",
            "* Q0 (the initial state)\n",
            "* Q1\n",
            "* Q2\n",
            "\n",
            "With the following transitions:\n",
            "\n",
            "* Q0 -> Q1 (when the system receives input \"A\")\n",
            "* Q1 -> Q2 (when the system receives input \"B\")\n",
            "* Q2 -> Q0 (when the system receives input \"C\")\n",
            "\n",
            "In this example, the states are Q0, Q1, and Q2, and the connections between them represent the possible transitions between those states.\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "oRALcjobCncI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e88089-2e4c-4549-b675-a12cb5731edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many states does a pentagon shaped automaton have \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many states does a pentagon-shaped automaton have?\n",
            "\n",
            "Input: arc from q2 to q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: arc from q2 to q1\n",
            "\n",
            "Input: how many arcs marked by 1 are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: how many arcs marked by 1 are there?\n",
            "\n",
            "Input: q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Take a deep breath and work on this problem step-by-step\n",
            "\n",
            "Input: Which transitions end in q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Which transitions end in q0?\n",
            "\n",
            "Input: Is there an arc between q0 and q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there an arc between q0 and q1?\n",
            "\n",
            "Input: If q2 is the final state the language accepted by the automaton is the same \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: If q2 is the final state, the language accepted by the automaton is the same\n",
            "\n",
            "Input: transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: transitions\n",
            "\n",
            "Input: Is there an arc between q1 and q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there an arc between q1 and q2?\n",
            "\n",
            "Input: q0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: q0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indirect Response"
      ],
      "metadata": {
        "id": "4KowioxaGCuJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qg7gzL-GCuJ"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "MwxxyzHZGCuJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af781d1f-acf3-4a27-dcc1-be1e1da2b0cf",
        "id": "rEyhRZ1tGCuJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What happens when a 0 comes to state q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What happens when a 0 comes to state q1\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton accepts a language allowing words composed of an odd number of zeros and ones\n",
            "\n",
            "Input: is there an arc from q0 to q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there an arc from state q0 to state q1?\n",
            "\n",
            "Input: q0 to q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: q0 to q4\n",
            "\n",
            "Input: Is 11100 part of the accepted language \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is 11100 a member of the acknowledged language set?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton accepts a language that permits word constructions comprising an odd number of zeros and ones.\n",
            "\n",
            "Input: is q0 linked to q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is q0 linked to q0?\n",
            "\n",
            "Input: Is there a transition from q0 to q5 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition from state q0 to state q5?\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "k-b_giFTGCuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotIR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"is there an arc from q0 to q1\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "YoWAjitCGCuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "X0i_yVF5GCuK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotIR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d36df31-7b17-4b9d-972d-a4f07ce85bb2",
        "id": "fQKkJBJDGCuK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What happens when a 0 comes to state q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the result of a zero symbol (0) entering state q1?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton recognizes a language composed of strings with an odd number of 0s and 1s\n",
            "\n",
            "Input: is there an arc from q0 to q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition between q0 and q1\n",
            "\n",
            "Input: q0 to q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a path from state q0 to state q4?\n",
            "\n",
            "Input: Is 11100 part of the accepted language \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Does the state 11100 belong to the accepted language set?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a language accepted by the automaton composed of an odd number of zeros and ones?\n",
            "\n",
            "Input: is q0 linked to q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a self-loop in q4?\n",
            "\n",
            "Input: Is there a transition from q0 to q5 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition between q0 and q5\n"
          ]
        }
      ],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotIR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "ElXDcQQiGCuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsIR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"is there an arc from q0 to q1\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "Example 2:\n",
        "Input: \"is q4 a final state\"\n",
        "Output: \"<What is q4>\"\n",
        "Example 3:\n",
        "Input: \"The automaton accepts a language allowing words made of an odd number of 0s and 1s\"\n",
        "Output: \"<The automaton's alphabet consists of words with an odd number of 0s and 1s>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bcLs1DorGCuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsIR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Indirect Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f0188f-02d5-40cc-9bef-d7f252a42838",
        "id": "blOqdvl4GCuL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you with that. Here's the translation of your input into controlled natural language format:\n",
            "\n",
            "Input: what are the states and the connections between states\n",
            "\n",
            "Output: <Specify the states and their transitions>\n",
            "\n",
            "In\n",
            "According to your understanding of Automaton 2 what is its optimal spatial representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsIR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "_o8MadqUGCuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c558ea-fd15-42ff-f8a3-00598b179b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What happens when a 0 comes to state q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the result of a 0 symbol entering state q1?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton recognizes a language composed of words with an odd number of 0s and 1s\n",
            "\n",
            "Input: is there an arc from q0 to q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition between q0 and q1\n",
            "\n",
            "Input: q0 to q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a path from state q0 to state q4?\n",
            "\n",
            "Input: Is 11100 part of the accepted language \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a word in the automaton's alphabet consisting of exactly five 0s and one 1?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton recognizes a language composed of strings with an odd number of 0s and 1s\n",
            "\n",
            "Input: is q0 linked to q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a self-loop in q0?\n",
            "\n",
            "Input: Is there a transition from q0 to q5 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition between q4 and q5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "Js0jIFtnGCuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ad6e3-4596-46c5-fb55-f55fc9d5c9b1",
        "id": "4DZNSoLoGCuM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you translate the user's input into a controlled natural language format suitable for an AIML chatbot. Here's my analysis and translation of the input:\n",
            "\n",
            "1. Analyze the user input for key components:\n",
            "Input: \"According to your understanding of Automaton 2 what is its optimal spatial representation?\"\n",
            "\n",
            "Keywords and phrases that signify specific automata concepts:\n",
            "\n",
            "* Automaton 2\n",
            "* Optimal spatial representation\n",
            "\n",
            "2. Identify critical elements and technical terminology\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "gQcIIcgPGCuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752020fa-6449-4f3c-daa9-683dcdc82520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What happens when a 0 comes to state q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: dfa-states-transition-description\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: is there an arc from q0 to q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: q0 to q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Is 11100 part of the accepted language \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: is q0 linked to q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Is there a transition from q0 to q5 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "v_hNWoqDGCuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fef8cb-9f6c-4d88-cfaa-7e38c3525a49",
        "id": "xreut7OfGCuM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your input into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<What are the states and the connections between states?>\n",
            "\n",
            "In the context of finite state automata, a \"state\" refers to a specific condition or status that the system can be in. These states are typically represented by nodes or circles in a diagram, and they are connected by arrows or transitions that represent the possible movements or changes between those states. The connections between states are called \"edges\" or \"transitions.\"\n",
            "\n",
            "For example, a finite state automaton might have the following states:\n",
            "\n",
            "* Q0 (the initial state)\n",
            "* Q1\n",
            "* Q2\n",
            "\n",
            "With the following transitions:\n",
            "\n",
            "* Q0 -> Q1 (when the system receives input \"A\")\n",
            "* Q1 -> Q2 (when the system receives input \"B\")\n",
            "* Q2 -> Q0 (when the system receives input \"C\")\n",
            "\n",
            "In this example, the states are Q0, Q1, and Q2, and the connections between them represent the possible transitions between those states.\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "UhGC_u2ZGCuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6589f5-32e0-4baf-b7fb-882b89609811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What happens when a 0 comes to state q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What happens when a 0 comes to state q1?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton accepts a language allowing words made of an odd number of 0s and 1s\n",
            "\n",
            "Input: is there an arc from q0 to q1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there an arc from q0 to q1?\n",
            "\n",
            "Input: q0 to q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: q0 to q4\n",
            "\n",
            "Input: Is 11100 part of the accepted language \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is 11100 part of the accepted language?\n",
            "\n",
            "Input: The automaton accepts a language allowing words made of an odd number of 0s and 1s \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The automaton accepts a language allowing words made of an odd number of 0s and 1s\n",
            "\n",
            "Input: is q0 linked to q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: is q0 linked to q0?\n",
            "\n",
            "Input: Is there a transition from q0 to q5 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Is there a transition from state q0 to state q5?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excess of Information"
      ],
      "metadata": {
        "id": "5pPWaDZAIesu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "allK3AWlIes1"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "goczDgpmIes1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f42f75c-a4f8-406d-db2d-5165e930c95f",
        "id": "l2Njq1sCIes1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many transitions start from q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many transitions start from state q0?\n",
            "\n",
            "Input: Please describe the transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Please provide information about the transitions between states.\n",
            "\n",
            "Input: describe the arcs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the arcs of the finite state machine, including their labels and the transitions they facilitate.\n",
            "\n",
            "Input: can you describe the transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the possible state transitions and their conditions?\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "-JjNzFW-Ies2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotEOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Please describe the transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "dxVYL-m4Ies2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "pSsBJAHwIes2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotEOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a97386-bac3-4740-f915-9ecafc3b7431",
        "id": "7uO8TtXSIes2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many transitions start from q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What is the number of transitions starting from initial state q0?\n",
            "\n",
            "Input: Please describe the transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are transitions\n",
            "\n",
            "Input: describe the arcs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the arcs\n",
            "\n",
            "Input: can you describe the transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the transition rules\n"
          ]
        }
      ],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotEOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "GLOw_5ftIes2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsEOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"Please describe the transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "Example 2:\n",
        "Input: \"how is marked the arc between q0 and q2\"\n",
        "Output: \"<Identify the label on the arc between q0 and q2>\"\n",
        "Example 3:\n",
        "Input: \"which is final state\"\n",
        "Output: \"<Tell me more about the final state>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RSj-QG2nIes2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsEOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Excess of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f0188f-02d5-40cc-9bef-d7f252a42838",
        "id": "62mOiCpbIes3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you with that. Here's the translation of your input into controlled natural language format:\n",
            "\n",
            "Input: what are the states and the connections between states\n",
            "\n",
            "Output: <Specify the states and their transitions>\n",
            "\n",
            "In\n",
            "According to your understanding of Automaton 2 what is its optimal spatial representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsEOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "GTVAfTt5Ies3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37be6a1a-6252-4a25-bd27-f0019d8af0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many transitions start from q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are the number of transitions starting from q0?\n",
            "\n",
            "Input: Please describe the transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are transitions\n",
            "\n",
            "Input: describe the arcs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the arcs\n",
            "\n",
            "Input: can you describe the transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What are transitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "dQz9-x8WIes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ad6e3-4596-46c5-fb55-f55fc9d5c9b1",
        "id": "XBk7eXFRIes3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you translate the user's input into a controlled natural language format suitable for an AIML chatbot. Here's my analysis and translation of the input:\n",
            "\n",
            "1. Analyze the user input for key components:\n",
            "Input: \"According to your understanding of Automaton 2 what is its optimal spatial representation?\"\n",
            "\n",
            "Keywords and phrases that signify specific automata concepts:\n",
            "\n",
            "* Automaton 2\n",
            "* Optimal spatial representation\n",
            "\n",
            "2. Identify critical elements and technical terminology\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "QOABsh_IIes3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e17d27d-040f-44b1-a302-4565ec2200ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many transitions start from q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: Please describe the transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: describe the arcs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: can you describe the transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "EK_2epeiIes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fef8cb-9f6c-4d88-cfaa-7e38c3525a49",
        "id": "La_PcXGKIes3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your input into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<What are the states and the connections between states?>\n",
            "\n",
            "In the context of finite state automata, a \"state\" refers to a specific condition or status that the system can be in. These states are typically represented by nodes or circles in a diagram, and they are connected by arrows or transitions that represent the possible movements or changes between those states. The connections between states are called \"edges\" or \"transitions.\"\n",
            "\n",
            "For example, a finite state automaton might have the following states:\n",
            "\n",
            "* Q0 (the initial state)\n",
            "* Q1\n",
            "* Q2\n",
            "\n",
            "With the following transitions:\n",
            "\n",
            "* Q0 -> Q1 (when the system receives input \"A\")\n",
            "* Q1 -> Q2 (when the system receives input \"B\")\n",
            "* Q2 -> Q0 (when the system receives input \"C\")\n",
            "\n",
            "In this example, the states are Q0, Q1, and Q2, and the connections between them represent the possible transitions between those states.\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "QVQ7JlXkIes3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c3babb-772a-4a76-a6c7-66bdffe06a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: How many transitions start from q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: How many transitions start from q0?\n",
            "\n",
            "Input: Please describe the transitions \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Please provide a description of the transitions in the finite state automaton.\n",
            "\n",
            "Input: describe the arcs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: describe the arcs\n",
            "\n",
            "Input: can you describe the transitions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Can you describe the transitions?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lake of Information"
      ],
      "metadata": {
        "id": "eHh0pPHLKAMP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONBZBTXKAMQ"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "NEYksLkcKAMQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54eb43b0-845e-4600-e619-f2f540500838",
        "id": "CAHF0GdmKAMQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What transitions enter and exit q4 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions enter and exit state Q4?\n",
            "\n",
            "Input: are there arcs between q2 and q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there arcs connecting state Q2 to state Q0?\n",
            "\n",
            "Input: What transitions enter and exit q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions enter and exit state q2?\n",
            "\n",
            "Input: Briefly describe the automaton How many states are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The finite state automaton has }briefly describe the automaton< with exactly }three states<.\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "PO7atMeqKAMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotLOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Briefly describe the automaton How many states are there\"\n",
        "Output: \"<Describe me briefly the automaton and tell me how many states are there>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "K4vps3KkKAMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d7350-7f92-4a09-81ca-bb96ab07034c",
        "id": "qMfSkxrVKAMR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your request into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<Tell me about the finite state machine.>\n",
            "\n",
            "This translation adheres to the specific vocabulary and syntax guidelines for an AIML chatbot, using technical terms related to finite state machines where appropriate. The request is clear and unambiguous, ensuring the chatbot can understand and respond accurately.\n",
            "Tell me a little bit about the automaton\n"
          ]
        }
      ],
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotLOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8084f868-5f1c-45fc-a2d8-56aef7e0f17a",
        "id": "wTebpdKRKAMS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What transitions enter and exit q4 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe to me the transitions that enter and exit state q4\n",
            "\n",
            "Input: are there arcs between q2 and q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there transitions from state q2 to state q0?\n",
            "\n",
            "Input: What transitions enter and exit q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the transitions that enter and exit state q2, please?\n",
            "\n",
            "Input: Briefly describe the automaton How many states are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe me briefly the automaton and tell me how many states it has\n"
          ]
        }
      ],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotLOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "ZfkS0Kx1KAMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsLOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"Briefly describe the automaton How many states are there\"\n",
        "Output: \"<Describe me briefly the automaton and tell me how many states are there>\"\n",
        "Example 2:\n",
        "Input: \"There is a transition between q2 and q0\"\n",
        "Output: \"<Is there a transition between q2 and q0>\"\n",
        "Example 3:\n",
        "Input: \"What transitions enter and exit q1\"\n",
        "Output: \"<List transitions entering and exiting q1>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9_rIKD7nKAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsLOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Lake of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f0188f-02d5-40cc-9bef-d7f252a42838",
        "id": "WPjH4YkSKAMT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you with that. Here's the translation of your input into controlled natural language format:\n",
            "\n",
            "Input: what are the states and the connections between states\n",
            "\n",
            "Output: <Specify the states and their transitions>\n",
            "\n",
            "In\n",
            "According to your understanding of Automaton 2 what is its optimal spatial representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsLOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "ZTEYg5tQKAMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b19775e-6663-45e7-f085-e6ae7702816a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What transitions enter and exit q4 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the transitions entering and exiting q4\n",
            "\n",
            "Input: are there arcs between q2 and q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there arcs between q2 and q0?\n",
            "\n",
            "Input: What transitions enter and exit q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe me the transitions entering and exiting q2\n",
            "\n",
            "Input: Briefly describe the automaton How many states are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe me briefly the automaton and tell me how many states are there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "QyNVRRwKKAMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ad6e3-4596-46c5-fb55-f55fc9d5c9b1",
        "id": "Fy_4VKxXKAMU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! I'd be happy to help you translate the user's input into a controlled natural language format suitable for an AIML chatbot. Here's my analysis and translation of the input:\n",
            "\n",
            "1. Analyze the user input for key components:\n",
            "Input: \"According to your understanding of Automaton 2 what is its optimal spatial representation?\"\n",
            "\n",
            "Keywords and phrases that signify specific automata concepts:\n",
            "\n",
            "* Automaton 2\n",
            "* Optimal spatial representation\n",
            "\n",
            "2. Identify critical elements and technical terminology\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "TkVv7TqgKAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83463d85-76f8-4ebc-f6a9-84002d36c9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What transitions enter and exit q4 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: are there arcs between q2 and q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n",
            "\n",
            "Input: What transitions enter and exit q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: ' and '\n",
            "\n",
            "Input: Briefly describe the automaton How many states are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Output not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "5eIwwwfPKAMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fef8cb-9f6c-4d88-cfaa-7e38c3525a49",
        "id": "qbEz4kUJKAMU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here's the translation of your input into controlled natural language suitable for an AIML chatbot:\n",
            "\n",
            "<What are the states and the connections between states?>\n",
            "\n",
            "In the context of finite state automata, a \"state\" refers to a specific condition or status that the system can be in. These states are typically represented by nodes or circles in a diagram, and they are connected by arrows or transitions that represent the possible movements or changes between those states. The connections between states are called \"edges\" or \"transitions.\"\n",
            "\n",
            "For example, a finite state automaton might have the following states:\n",
            "\n",
            "* Q0 (the initial state)\n",
            "* Q1\n",
            "* Q2\n",
            "\n",
            "With the following transitions:\n",
            "\n",
            "* Q0 -> Q1 (when the system receives input \"A\")\n",
            "* Q1 -> Q2 (when the system receives input \"B\")\n",
            "* Q2 -> Q0 (when the system receives input \"C\")\n",
            "\n",
            "In this example, the states are Q0, Q1, and Q2, and the connections between them represent the possible transitions between those states.\n",
            "what are the states and the connections between states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "txRetTpwKAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e620849-742e-4a28-964c-1345a62d5e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: What transitions enter and exit q4 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions enter and exit q4?\n",
            "\n",
            "Input: are there arcs between q2 and q0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Are there arcs between state q2 and state q0?\n",
            "\n",
            "Input: What transitions enter and exit q2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What transitions enter and exit state q2?\n",
            "\n",
            "Input: Briefly describe the automaton How many states are there \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: Describe the automaton and its number of states, if possible in a single sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LPDcbVr2C-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7554cfc8ce65465b98da9c69c126413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7211f954ae4c4cbab701a26718c49bbc",
              "IPY_MODEL_a77067eaecd545c689023726d4f317e5",
              "IPY_MODEL_ffcca6087acb4601a4f22378281b68cd"
            ],
            "layout": "IPY_MODEL_e331a08a1174423291a16f0211d23282"
          }
        },
        "7211f954ae4c4cbab701a26718c49bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22887fb8a161426f82d774aed4631401",
            "placeholder": "​",
            "style": "IPY_MODEL_f6de9c5b2e684ae99f2ea1270a850321",
            "value": "llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "a77067eaecd545c689023726d4f317e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77f38feb0bb4653857e8fa0db83c0c1",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f83e568dcf824fc8adeaac9a5617b397",
            "value": 9763701888
          }
        },
        "ffcca6087acb4601a4f22378281b68cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63601f51389b417e90005a2e6838631d",
            "placeholder": "​",
            "style": "IPY_MODEL_1d8df51bb1534b599c40b4c45a546fab",
            "value": " 9.76G/9.76G [01:14&lt;00:00, 129MB/s]"
          }
        },
        "e331a08a1174423291a16f0211d23282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22887fb8a161426f82d774aed4631401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6de9c5b2e684ae99f2ea1270a850321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a77f38feb0bb4653857e8fa0db83c0c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83e568dcf824fc8adeaac9a5617b397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63601f51389b417e90005a2e6838631d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8df51bb1534b599c40b4c45a546fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}