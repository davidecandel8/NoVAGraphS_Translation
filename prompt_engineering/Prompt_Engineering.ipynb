{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58XL8olWy4Gt"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz_tLUjZzFm8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Installazione delle librerie necessarie con specifiche per l'uso della GPU\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install openpyxl\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCrawMYnz2J0"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import spacy\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXLvFoRHz9wx"
      },
      "source": [
        "# Corpus Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVW0iox70HJv"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "data_path = '/content/drive/My Drive/Llama2_Translation_LNC/interaction-corpus.xlsx'\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otbgFgP51CLw"
      },
      "outputs": [],
      "source": [
        "# Filter the data for system responses only\n",
        "system_responses = data[data['Participant'] == 'S']\n",
        "\n",
        "# Count the types of errors for system responses\n",
        "system_error_counts = system_responses['Errors_GOLD'].value_counts()\n",
        "system_error_counts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data for system responses only\n",
        "user_responses = data[data['Participant'] == 'U']\n",
        "\n",
        "# Count the types of errors for system responses\n",
        "system_error_counts = user_responses['Errors_GOLD'].value_counts()\n",
        "system_error_counts"
      ],
      "metadata": {
        "id": "6gRFI14JqEbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escludiamo:\n",
        "1. Off-topic\n",
        "3. Non-understandable\n",
        "4. Non-cooperativity\n",
        "5. Repetition"
      ],
      "metadata": {
        "id": "GOTcPQR3s1ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Change"
      ],
      "metadata": {
        "id": "MfiKSzSKp1AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Topic change\" error\n",
        "topic_change_indices = system_responses[system_responses['Errors_GOLD'] == 'Topic change'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Topic Change\"\n",
        "user_inputs_before_topic_change = data.loc[topic_change_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_tc = user_inputs_before_topic_change[\n",
        "    user_inputs_before_topic_change['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_topic_change['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_tc = filtered_user_inputs_tc['Text'].sample(15).tolist()\n",
        "final_user_queries_tc"
      ],
      "metadata": {
        "id": "sJEMw1EFv31M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_tc)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "id": "C3O4cG0zw2Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Straight Wrong Response"
      ],
      "metadata": {
        "id": "CtC0h53Kp7cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Straight wrong response\" error\n",
        "straight_wrong_response_indices = system_responses[system_responses['Errors_GOLD'] == 'Straight wrong response'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Straight wrong response\"\n",
        "user_inputs_before_straight_wrong_response = data.loc[straight_wrong_response_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_swr = user_inputs_before_straight_wrong_response[\n",
        "    user_inputs_before_straight_wrong_response['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_straight_wrong_response['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_swr = filtered_user_inputs_swr['Text'].sample(10).tolist()\n",
        "final_user_queries_swr"
      ],
      "metadata": {
        "id": "fQju8EP7govb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_swr)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "id": "yJqTmkb_pbdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_swr['Text'].tolist()"
      ],
      "metadata": {
        "id": "Xf_JViCzvpdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indirect Response"
      ],
      "metadata": {
        "id": "yC7agCVwqDEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Indirect response\" error\n",
        "indirect_response_indices = system_responses[system_responses['Errors_GOLD'] == 'Indirect response'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Indirect response\"\n",
        "user_inputs_before_indirect_response = data.loc[indirect_response_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_ir = user_inputs_before_indirect_response[\n",
        "    user_inputs_before_indirect_response['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_indirect_response['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_ir = filtered_user_inputs_ir['Text'].sample(8).tolist()\n",
        "final_user_queries_ir"
      ],
      "metadata": {
        "id": "CUSmiG0Zip1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_ir)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "id": "HzrkQ4sIpe8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_ir['Text'].tolist()"
      ],
      "metadata": {
        "id": "3x9BYaZb4PHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excess of Information"
      ],
      "metadata": {
        "id": "bN9ns0frqNwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Excess of information\" error\n",
        "excess_of_information_indices = system_responses[system_responses['Errors_GOLD'] == 'Excess of information'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Excess of information\"\n",
        "user_inputs_before_excess_of_information = data.loc[excess_of_information_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_eoi = user_inputs_before_excess_of_information[\n",
        "    user_inputs_before_excess_of_information['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_excess_of_information['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_eoi = filtered_user_inputs_eoi['Text'].sample(4).tolist()\n",
        "final_user_queries_eoi"
      ],
      "metadata": {
        "id": "NH45Lb57cFF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_eoi)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "id": "Wj25QnJbpiSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_eoi['Text'].tolist()"
      ],
      "metadata": {
        "id": "d3t-1EWN52dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lack of Information"
      ],
      "metadata": {
        "id": "kLQs11WDqUhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rows where the system response has a \"Lack of information\" error\n",
        "lack_of_information_indices = system_responses[system_responses['Errors_GOLD'] == 'Lack of information'].index\n",
        "\n",
        "# Get the corresponding user inputs just before the system responses with \"Lack of information\"\n",
        "user_inputs_before_lack_of_information = data.loc[lack_of_information_indices - 1]\n",
        "\n",
        "# Define the accepted user errors\n",
        "accepted_user_errors = ['Ignoring question/feedback', 'Grammatical error', 'Lack of information', 'Ill-formed']\n",
        "\n",
        "# Filter the user inputs to include only those with the specified Errors_GOLD\n",
        "filtered_user_inputs_loi = user_inputs_before_lack_of_information[\n",
        "    user_inputs_before_lack_of_information['Errors_GOLD'].isin(accepted_user_errors) | user_inputs_before_lack_of_information['Errors_GOLD'].isnull()\n",
        "]\n",
        "# Extract only the text of the queries\n",
        "final_user_queries_loi = filtered_user_inputs_loi['Text'].sample(4).tolist()\n",
        "final_user_queries_loi"
      ],
      "metadata": {
        "id": "vWBAxP_ojJCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolare la lunghezza dell'array matched_queries_updated\n",
        "array_length = len(filtered_user_inputs_loi)\n",
        "print(\"La lunghezza dell'array è:\", array_length)"
      ],
      "metadata": {
        "id": "p9eFISkEpmwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_inputs_loi['Text'].tolist()"
      ],
      "metadata": {
        "id": "SEZVIIqR552k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdrW2k_4wNRf"
      },
      "source": [
        "# Chiamata al modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P5evo50wZLk"
      },
      "outputs": [],
      "source": [
        "# Seleziono LLama2 13B come modello\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "# Download del modello da Hugging Face Hub\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# Inizializzazione del modello con configurazione per GPU\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # Numero di core della CPU\n",
        "    n_batch=512,  # Numero di batch, dipende dalla VRAM della GPU\n",
        "    n_gpu_layers=32  # Numero di layer gestiti dalla GPU\n",
        ")\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-SlHHjXwdaC"
      },
      "outputs": [],
      "source": [
        "# Generazione della risposta\n",
        "def call_model(prompt):\n",
        "  response = lcpp_llm(prompt=prompt, max_tokens=4096)\n",
        "  #print(response[\"choices\"][0][\"text\"])\n",
        "  return response[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwOMqxWiTZng"
      },
      "source": [
        "# Salvataggio Dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBBHDd4npeD"
      },
      "outputs": [],
      "source": [
        "def setup_xlsx(file_path):\n",
        "    # Controlla se il file esiste\n",
        "    if not os.path.exists(file_path):\n",
        "        # Crea un nuovo workbook e aggiunge un foglio\n",
        "        wb = openpyxl.Workbook()\n",
        "        ws = wb.active\n",
        "        ws.title = \"Data\"\n",
        "        ws.append(['Input', 'Translation', 'Output', 'Type', 'Error'])  # Aggiungi intestazioni\n",
        "        wb.save(file_path)\n",
        "    else:\n",
        "        # Apri il workbook esistente\n",
        "        wb = openpyxl.load_workbook(file_path)\n",
        "    return wb\n",
        "\n",
        "def append_to_xlsx(wb, file_path, data):\n",
        "    # Carica il foglio di lavoro\n",
        "    ws = wb.active\n",
        "    # Aggiungi i nuovi dati\n",
        "    ws.append(data)\n",
        "    # Salva il workbook\n",
        "    wb.save(file_path)\n",
        "\n",
        "# Percorso del file Excel\n",
        "filename = '/content/drive/My Drive/Llama2_Translation_LNC/Llama2InteractionCorpus_01_05_IR_EOI_LOI.xlsx'\n",
        "\n",
        "# Setup iniziale del file Excel\n",
        "wb = setup_xlsx(filename)\n",
        "\n",
        "def update_excel(input_text, translation, output, prompt_type, error_type):\n",
        "    # Dati da aggiungere\n",
        "    data = [input_text, translation, output, prompt_type, error_type]\n",
        "    # Aggiorna il file Excel\n",
        "    append_to_xlsx(wb, filename, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNBshkVc50JZ"
      },
      "source": [
        "# Prompt Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_between_brackets(input_string):\n",
        "    # Usa un'espressione regolare per trovare il testo tra < e >\n",
        "    match = re.search(r'<([^>]*)>', input_string)\n",
        "    # Se c'è una corrispondenza, restituisci il gruppo catturato, altrimenti restituisci None o una stringa vuota\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return \"Output not found!\""
      ],
      "metadata": {
        "id": "4HSWwSm8sUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNUyD0Rk8iPd"
      },
      "outputs": [],
      "source": [
        "llama2PromptTemplate = lambda userPrompt,systemPrompt: f\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "{systemPrompt}\n",
        "<</SYS>>\n",
        "\n",
        "{userPrompt} [/INST]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ794IHY7AN3"
      },
      "outputs": [],
      "source": [
        "systemPrompt=\"\"\"\n",
        "You are an assistant expert on finite state automata. Your task is to translate \\\n",
        "user inputs from natural language to a controlled natural language format. \\\n",
        "This translation should adhere to specific vocabulary and syntax guidelines \\\n",
        "suitable for an AIML chatbot. Your response must be precise, using technical \\\n",
        "terminology related to finite state automata where appropriate. Ensure the \\\n",
        "translated text contains clear, unambiguous language that aligns with the \\\n",
        "rules governing the AIML chatbot's response system. Focus on maintaining the \\\n",
        "integrity of the technical content while simplifying the explanation for \\\n",
        "accurate and effective chatbot communication.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYAM_B577Gc4"
      },
      "outputs": [],
      "source": [
        "userPromptZeroShot = lambda text: f\"\"\"\n",
        "  Translate the text delimited by triple backticks, without any preambles or \\\n",
        "  additional explanation, into controlled natural language sentence suitable \\\n",
        "  for an AIML system.\n",
        "  Format the response by directly placing the translation within angle \\\n",
        "  brackets < >.\n",
        "  For example:\n",
        "  Output: <Your text here>.\n",
        "  Now, use the same format for the translation:\n",
        "  ``` {text} ```\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptChainOfThoughts = lambda text: f\"\"\"\n",
        "Translate '{text}' into controlled natural language for an AIML chatbot, focusing on:\n",
        "1. Identifying key components related to finite state automata.\n",
        "2. Using precise, technical terminology where appropriate.\n",
        "3. Ensuring clarity and adherence to AIML chatbot rules.\n",
        "Ensure that only the response itself is literally placed between '<' and '>'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y6XRaPOvtv3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptChainOfThoughts = lambda text: f\"\"\"\n",
        "Translate the user's input considering the following steps:\n",
        "1. Analyze the user input for key components:\n",
        "   Input: '{text}'\n",
        "2. Identify the critical elements and any technical terminology related to finite state automata:\n",
        "   - Examine keywords and phrases that signify specific automata concepts.\n",
        "3. Construct a response in controlled natural language:\n",
        "   - Determine how to express these components clearly and unambiguously.\n",
        "   - Use controlled language structures that fit the AIML chatbot's rules.\n",
        "4. Synthesize the translation to ensure clarity, precision, and adherence to technical terms.\n",
        "5. Finalize the translation ensuring technical accuracy and alignment with AIML standards:\n",
        "   - Format the response to be enclosed within angular brackets.\n",
        "6. Explicit instruction for output:\n",
        "   - Example format for final output: 'Please format the final response like this: <RESPONSE>'\n",
        "   - Ensure that only the response itself is literally placed between '<' and '>'.\n",
        "Example:\n",
        "Input: \"Tell me a little bit about the automaton \"\n",
        "Output: \"<Describe me briefly the automaton>\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DuEx1bRpRqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptZeroChainOfThoughts = lambda text: f\"\"\"\n",
        "  Translate the text delimited by triple backticks, without any preambles or \\\n",
        "  additional explanation, into controlled natural language sentence suitable \\\n",
        "  for an AIML system.\n",
        "  Format the response by directly placing the translation within angle \\\n",
        "  brackets < >.\n",
        "  For example:\n",
        "  Output: <Your text here>.\n",
        "  Now the same format for the translation:\n",
        "  ``` {text} ```\n",
        "  Take a deep breath and work on this problem step-by-step.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "atVI6bXsvptw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Change"
      ],
      "metadata": {
        "id": "sDJwiSnWvaSs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viec3G8N6AKu"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPJ02sJ__UCW"
      },
      "outputs": [],
      "source": [
        "text = \"Tell me a little bit about the automaton\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK7L-lIN-J0n"
      },
      "outputs": [],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "HD85Wby4DELy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotTC = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Tell me a little bit about the automaton \"\n",
        "Output: \"<Describe me briefly the automaton>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "96tx-ydwV2if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1WN_t48y6t5"
      },
      "outputs": [],
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotTC(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw4Uefxny6uD"
      },
      "outputs": [],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotTC(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Topic Change\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "kWgIqGK0f8bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:\n",
        "Input: \"Tell me a little bit about the automaton\"\n",
        "Output: \"Describe me briefly the automaton>\"\n",
        "\n",
        "Example 2:\n",
        "Input: \"According to your understanding of Automaton 2...\"\n",
        "Output: \"Tell me more about the automaton 2>\"\n",
        "\n",
        "Example 3:\n",
        "Input: \"What is the final state of an automaton\"\n",
        "Output: \"Identify the final state of the automaton>\"\n",
        "\n",
        "Example 4:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"What are the transitions>\"\n",
        "\n",
        "Example 5:\n",
        "Input: \"How is an automaton defined\"\n",
        "Output: \"What is an automaton>\"\n",
        "\n",
        "Example 6:\n",
        "Input: \"what are the symbols accepted by the automaton\"\n",
        "Output: \"Specify the alphabet used by the automaton.>\"\n",
        "\n",
        "Example 7:\n",
        "Input: \"nodes\"\n",
        "Output: \"Tell me more about states>\"\n",
        "\n",
        "Example 8:\n",
        "Input: \"What are the transitions\"\n",
        "Output: \"What are transitions>\"\n",
        "\n",
        "Example 9:\n",
        "Input: \"Give me a list of the arcs\"\n",
        "Output: \"What are arcs>\"\n",
        "\n",
        "Example 10:\n",
        "Input: \"what is the initial stage of the automaton\"\n",
        "Output: \"What is the initial state>\""
      ],
      "metadata": {
        "id": "KASw9-nH_eI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsTC = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"According to your understanding of Automaton 2...\"\n",
        "Output: \"<Tell me more about the automaton 2>\"\n",
        "Example 2:\n",
        "Input: \"What is the final state of an automaton\"\n",
        "Output: \"<Identify the final state of the automaton>\"\n",
        "Example 3:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"<What are the transitions>\"\n",
        "Example 4:\n",
        "Input: \"How is an automaton defined\"\n",
        "Output: \"<What is an automaton>\"\n",
        "Example 5:\n",
        "Input: \"what are the symbols accepted by the automaton\"\n",
        "Output: \"<Specify the alphabet used by the automaton.>\"\n",
        "Example 6:\n",
        "Input: \"nodes\"\n",
        "Output: \"<Tell me more about states>\"\n",
        "Example 7:\n",
        "Input: \"Give me a list of the arcs\"\n",
        "Output: \"<What are arcs>\"\n",
        "Example 8:\n",
        "Input: \"what is the initial stage of the automaton\"\n",
        "Output: \"<What is the initial state>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bfi8urM5W-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"According to your understanding of Automaton 2 what is its optimal spatial representation\"\n",
        "print(\"\\nInput: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsTC(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "__KPGGfSoVMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsTC(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "xx6CV0dcAgsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "xPmkMLVcwyRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "print(\"Completed Output: \" + model_output)\n",
        "#update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "JEWfdFzZQygQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "_6Zy-ZkrBh8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "sNpQguQPw8zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"what are the states and the connections between states\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "8h8mNO-fYPvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_tc:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Topic Change\")"
      ],
      "metadata": {
        "id": "jdTP_25rCJ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Straight Wrong Response"
      ],
      "metadata": {
        "id": "docJqTtpCncC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuUmnbmbCncD"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx3zPmmQCncD"
      },
      "outputs": [],
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvHMe5kXCncE"
      },
      "outputs": [],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "sXKvC5CfCncF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotSWR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"is q4 linked to q0\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zcW104EeCncF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_bmHm3PCncF"
      },
      "outputs": [],
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotSWR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGAI08GfCncG"
      },
      "outputs": [],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotSWR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Straight Wrong Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "CH5YWYmUCncG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsSWR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"is q4 linked to q0\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "Example 2:\n",
        "Input: \"transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "Example 3:\n",
        "Input: \"How are transitions defined\"\n",
        "Output: \"<What are the transitions>\"\n",
        "Example 4:\n",
        "Input: \"arc from q2 to q1\"\n",
        "Output: \"<Is there a transition between q2 and q1>\"\n",
        "Example 5:\n",
        "Input: \"q0\"\n",
        "Output: \"<What is q0>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rMQKEyE1CncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsSWR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "IKsW9IcOCncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsSWR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "9Xgl_IqsCncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "S9fvnnBlCncH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "KF1X8qpPCncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "f_sdogSLCncI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "pGkwf5upCncI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is there an arc between q0 and q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "Ns6gQ70HCncI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_swr:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Straight Wrong Response\")"
      ],
      "metadata": {
        "id": "oRALcjobCncI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indirect Response"
      ],
      "metadata": {
        "id": "4KowioxaGCuJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qg7gzL-GCuJ"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwxxyzHZGCuJ"
      },
      "outputs": [],
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEyhRZ1tGCuJ"
      },
      "outputs": [],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "k-b_giFTGCuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotIR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"is there an arc from q0 to q1\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "YoWAjitCGCuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0i_yVF5GCuK"
      },
      "outputs": [],
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotIR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQKkJBJDGCuK"
      },
      "outputs": [],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotIR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Indirect Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "ElXDcQQiGCuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsIR = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"is there an arc from q0 to q1\"\n",
        "Output: \"<Is there a transition between q4 and q0>\"\n",
        "Example 2:\n",
        "Input: \"is q4 a final state\"\n",
        "Output: \"<What is q4>\"\n",
        "Example 3:\n",
        "Input: \"The automaton accepts a language allowing words made of an odd number of 0s and 1s\"\n",
        "Output: \"<The automaton's alphabet consists of words with an odd number of 0s and 1s>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bcLs1DorGCuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsIR(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "blOqdvl4GCuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsIR(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "_o8MadqUGCuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "Js0jIFtnGCuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "4DZNSoLoGCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "gQcIIcgPGCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "v_hNWoqDGCuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"is there an arc from q0 to q1\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "xreut7OfGCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_ir:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Indirect Response\")"
      ],
      "metadata": {
        "id": "UhGC_u2ZGCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excess of Information"
      ],
      "metadata": {
        "id": "5pPWaDZAIesu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "allK3AWlIes1"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goczDgpmIes1"
      },
      "outputs": [],
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2Njq1sCIes1"
      },
      "outputs": [],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "-JjNzFW-Ies2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotEOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Please describe the transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "dxVYL-m4Ies2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSsBJAHwIes2"
      },
      "outputs": [],
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotEOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uO8TtXSIes2"
      },
      "outputs": [],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotEOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Excess of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "GLOw_5ftIes2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsEOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"Please describe the transitions\"\n",
        "Output: \"<What are transitions>\"\n",
        "Example 2:\n",
        "Input: \"how is marked the arc between q0 and q2\"\n",
        "Output: \"<Identify the label on the arc between q0 and q2>\"\n",
        "Example 3:\n",
        "Input: \"which is final state\"\n",
        "Output: \"<Tell me more about the final state>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RSj-QG2nIes2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsEOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "62mOiCpbIes3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsEOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "GTVAfTt5Ies3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "dQz9-x8WIes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "XBk7eXFRIes3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "QOABsh_IIes3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "EK_2epeiIes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how is marked the arc between q0 and q2\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "La_PcXGKIes3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_eoi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Excess of Information\")"
      ],
      "metadata": {
        "id": "QVQ7JlXkIes3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lake of Information"
      ],
      "metadata": {
        "id": "eHh0pPHLKAMP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONBZBTXKAMQ"
      },
      "source": [
        "### Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEYksLkcKAMQ"
      },
      "outputs": [],
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAHF0GdmKAMQ"
      },
      "outputs": [],
      "source": [
        "# Zero-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroShot(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Shot"
      ],
      "metadata": {
        "id": "PO7atMeqKAMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptOneShotLOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the example:\n",
        "\n",
        "Example:\n",
        "Input: \"Briefly describe the automaton How many states are there\"\n",
        "Output: \"<Describe me briefly the automaton and tell me how many states are there>\"\n",
        "\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "K4vps3KkKAMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMfSkxrVKAMR"
      },
      "outputs": [],
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptOneShotLOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"One Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTebpdKRKAMS"
      },
      "outputs": [],
      "source": [
        "# One-shot prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptOneShotLOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"One Shot\", \"Lake of Information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shots"
      ],
      "metadata": {
        "id": "ZfkS0Kx1KAMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userPromptFewShotsLOI = lambda text: f\"\"\"\n",
        "Please translate the following inputs into controlled natural language.\n",
        "Use the format provided in the examples:\n",
        "Example 1:\n",
        "Input: \"Briefly describe the automaton How many states are there\"\n",
        "Output: \"<Describe me briefly the automaton and tell me how many states are there>\"\n",
        "Example 2:\n",
        "Input: \"There is a transition between q2 and q0\"\n",
        "Output: \"<Is there a transition between q2 and q0>\"\n",
        "Example 3:\n",
        "Input: \"What transitions enter and exit q1\"\n",
        "Output: \"<List transitions entering and exiting q1>\"\n",
        "Now, translate the new input using the same controlled natural language format.\n",
        "Format the response by directly placing the translation within angle brackets < >.\n",
        "Input: {text}\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9_rIKD7nKAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptFewShotsLOI(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Few Shots\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "WPjH4YkSKAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shots prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptFewShotsLOI(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Few Shots\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "ZTEYg5tQKAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thoughts"
      ],
      "metadata": {
        "id": "QyNVRRwKKAMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "Fy_4VKxXKAMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "TkVv7TqgKAMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero Chain of Thoughts"
      ],
      "metadata": {
        "id": "5eIwwwfPKAMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What transitions enter and exit q0\"\n",
        "print(\"Input: \" + text)\n",
        "model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "translation = extract_between_brackets(model_output)\n",
        "print(\"Output: \" + translation)\n",
        "update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "qbEz4kUJKAMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Chain of Thoughts prompt focused on translating into controlled natural language\n",
        "for query in final_user_queries_loi:\n",
        "  text = query\n",
        "  print(\"\\nInput: \" + text)\n",
        "  model_output = call_model(llama2PromptTemplate(userPromptZeroChainOfThoughts(text), systemPrompt))\n",
        "  translation = extract_between_brackets(model_output)\n",
        "  print(\"Output: \" + translation)\n",
        "  update_excel(text, translation, model_output, \"Zero Chain of Thoughts\", \"Lake of Information\")"
      ],
      "metadata": {
        "id": "txRetTpwKAMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LPDcbVr2C-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}